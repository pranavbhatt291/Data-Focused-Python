{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pansul47\\Anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import math\n",
    "# Use svg backend for better quality\n",
    "import matplotlib\n",
    "# AUTOLAB_IGNORE_START\n",
    "matplotlib.use(\"svg\")\n",
    "# AUTOLAB_IGNORE_STOP\n",
    "import matplotlib.pyplot as plt\n",
    "# AUTOLAB_IGNORE_START\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0) # you should adjust this to fit your screen\n",
    "# AUTOLAB_IGNORE_STOP\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api key :  YvwhXRu49HCL6doJk7GyYOKiT  \n",
      "api secret :  hgY1Gg1U07Q9Ny0g42xYHOBWHbX27RMcv6d6CeHllgpMW3HHOY  \n",
      "access key :  977356129805139968-0xZhzoPjeJMJvYrU4fiz94ejYPZVixe  \n",
      "access secret :  dtMA0EjdTemD6RVvpLjJXp4KMBS5JPDcLJsZ63jxGPsf0\n"
     ]
    }
   ],
   "source": [
    "def read_api_key(filepath1, filepath2, filepath3, filepath4):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filepath1, 'r') as f:\n",
    "        api_key = f.read().replace('\\n','')\n",
    "    with open(filepath2, 'r') as f:\n",
    "        api_secret = f.read().replace('\\n','')\n",
    "    with open(filepath3, 'r') as f:\n",
    "        access_token_key = f.read().replace('\\n','')\n",
    "    with open(filepath4, 'r') as f:\n",
    "        access_token_secret = f.read().replace('\\n','')\n",
    "    \n",
    "    return (api_key, api_secret, access_token_key, access_token_secret)     \n",
    "    \n",
    "api_key, api_secret, access_token_key, access_token_secret = read_api_key(\"api_key.txt\", \"api_secret.txt\", \n",
    "                                                                          \"access_token_key.txt\", \"access_token_secret.txt\")\n",
    "print(\"api key : \", api_key, \" \\napi secret : \", api_secret, \" \\naccess key : \", access_token_key,\n",
    "      \" \\naccess secret : \", access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[['api_key = \"YvwhXRu49HCL6doJk7GyYOKiT\"'],\n",
    " ['api_secret = \"hgY1Gg1U07Q9Ny0g42xYHOBWHbX27RMcv6d6CeHllgpMW3HHOY\"'],\n",
    " ['access_token_key = \"977356129805139968-0xZhzoPjeJMJvYrU4fiz94ejYPZVixe\"'],\n",
    " ['access_token_secret = \"dtMA0EjdTemD6RVvpLjJXp4KMBS5JPDcLJsZ63jxGPsf0\"']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API\n",
    "\n",
    "Simple use of the request library by managing the search_headers and search_params for searching and authenticating all get requrest for popular tweets. We will try to get 10 most popular tweets for the topics related to \"facebook\"\n",
    "\n",
    "https://speca.io/speca/twitter-rest-api-v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "key_secret = '{}:{}'.format(api_key, api_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_url = '{}oauth2/token'.format(base_url)\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "}\n",
    "\n",
    "auth_data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status code okay\n",
    "auth_resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_type', 'access_token'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keys in data response are token_type (bearer) and access_token (your access token)\n",
    "auth_resp.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = auth_resp.json()['access_token']\n",
    "search_headers = {\n",
    "    'Authorization': 'Bearer {}'.format(access_token)    \n",
    "}\n",
    "\n",
    "search_params = {\n",
    "    'q': 'facebook',\n",
    "    'lang': 'en',\n",
    "    'result_type': 'popular',\n",
    "    'count': 10\n",
    "}\n",
    "\n",
    "search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "\n",
    "search_resp = requests.get(search_url, headers=search_headers, params=search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://t.co/9756IWue7U\n",
      "0\n",
      "No URL Found, Move on !\n",
      "1\n",
      "https://t.co/KYH1C6EH9X\n",
      "1\n",
      "https://t.co/dQycX0ZLxe\n",
      "1\n",
      "https://t.co/GMjzdd577P\n",
      "1\n",
      "https://t.co/ZvAzQbgAab\n",
      "1\n",
      "https://t.co/MM7puBeUIy\n",
      "1\n",
      "https://t.co/SEvXlL5D1W\n",
      "1\n",
      "https://t.co/L5L423GYoN\n",
      "1\n",
      "https://t.co/321ylea87T\n"
     ]
    }
   ],
   "source": [
    "tweet_data = search_resp.json()\n",
    "\n",
    "for x in tweet_data['statuses']:\n",
    "#     print(x['text'])\n",
    "#     print(x['entities']['urls'])\n",
    "    print(len(x['entities']['urls']))\n",
    "    if int(len(x['entities']['urls'])) == 0:\n",
    "        print(\"No URL Found, Move on !\")\n",
    "    else:\n",
    "        print(x['entities']['urls'][0]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API\n",
    "\n",
    "Manage pagination and rate limiting by using \"Tweepy\" which will increase the request to 450 per 15 minuites (less than 180 requests per 15 minuites). Also, simple use of the geocoding functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token_key, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "# if (not api):\n",
    "#     print(\"Faliure to Authenticate Handler\")\n",
    "#     sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @WINKONUSA: [OFFICIAL] 180329 #WINNER X genie MV BEHIND THE SCENE (모임ver.) #위너\n",
      "\n",
      "https://t.co/TJPhfqVD7h https://t.co/4BSc5XnfAT\n",
      "RT @WINKONUSA: [OFFICIAL] 180329 #WINNER X genie MV BEHIND THE SCENE (모임ver.) #위너\n",
      "\n",
      "https://t.co/TJPhfqVD7h https://t.co/4BSc5XnfAT\n",
      "Retweeted Kelsall School (@KelsallSchool):\n",
      "\n",
      "Hi everyone @TheLCUK - We would love you to come and see @KelsallSchool… https://t.co/QJFNkgnA28\n",
      "Retweeted Kelsall School (@KelsallSchool):\n",
      "\n",
      "Hi everyone @TheLCUK - We would love you to come and see @KelsallSchool… https://t.co/QJFNkgnA28\n",
      "I posted a new video to Facebook https://t.co/mQu4l7OEuB\n",
      "I posted a new video to Facebook https://t.co/mQu4l7OEuB\n",
      "RT @ChatterjeeSue: I got assaulted today. #TorontoPolice wont help me. Could you help me please, Honourable @Kathleen_Wynne , Prime Ministe…\n",
      "RT @ChatterjeeSue: I got assaulted today. #TorontoPolice wont help me. Could you help me please, Honourable @Kathleen_Wynne , Prime Ministe…\n",
      "Take sometime and listen to this. Every Wednesday @pruvit offers a live Facebook to help answer some questions. The… https://t.co/j8rIb5jrYC\n",
      "Take sometime and listen to this. Every Wednesday @pruvit offers a live Facebook to help answer some questions. The… https://t.co/j8rIb5jrYC\n"
     ]
    }
   ],
   "source": [
    "fetched_tweets = api.search(q = 'facebook', lang='en', show_user=True, count = 5)\n",
    "for one_tweet in fetched_tweets:\n",
    "    print(one_tweet.text)\n",
    "#     print(one_tweet.source_url)\n",
    "#     print(one_tweet._json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BreakingSkagit\n",
      " ------------------ \n",
      "1 On Thursdays there are lots of opportunities to take your kids out to an activity and meet other moms. Check them out on the calendar: https://t.co/d4hBa2J62v https://t.co/agCG4KPDLc\n",
      " ------------------ \n",
      "JaneWar41788991\n",
      " ------------------ \n",
      "2 Facebook refuse to remove post from Tory-supporting page calling for Jeremy Corbyn to be assassinated https://t.co/kJHurNI0HX via @EvolvePolitics\n",
      " ------------------ \n",
      "NicBuzzworthy\n",
      " ------------------ \n",
      "3 I post on Facebook like once every 3 months just to give people a chance to listen to me now and to prepare them for when the book deal comes\n",
      " ------------------ \n",
      "Downloaded 0 tweets\n"
     ]
    }
   ],
   "source": [
    "import jsonpickle\n",
    "#Maximum number of tweets we want to collect maximum tweets to extract 1000000\n",
    "maxTweets = 100000\n",
    "#The twitter Search API allows up to 100 tweets per query\n",
    "tweetsPerQry = 100\n",
    "tweet_saved = []\n",
    "tweet_saved_full = []\n",
    "# retweet_saved = []\n",
    "counter = 0\n",
    "# search_terms = ('superbowl OR super bowl OR #superbowl')\n",
    "\n",
    "#Open a text file to save the tweets to\n",
    "with open('facenook_related_tweets.csv', 'w') as f:\n",
    "\n",
    "    #Tell the Cursor method that we want to use the Search API (api.search)\n",
    "    #Also tell Cursor our query, and the maximum number of tweets to return\n",
    "    for tweet in tweepy.Cursor(api.search, q='facebook', lang=\"en\", tweet_mode=\"extended\").items(3) :         \n",
    "        counter = counter + 1\n",
    "        print(tweet.author.screen_name)\n",
    "        tweet_saved.append(tweet)\n",
    "        print(\" ------------------ \")\n",
    "        if tweet.retweeted != False:\n",
    "            print(tweet.retweeted_status.full_text)\n",
    "            tweet_saved_full.append(tweet.retweeted_status.full_text)\n",
    "        else:\n",
    "            print(counter, tweet.full_text)\n",
    "            tweet_saved_full.append(tweet.full_text)\n",
    "        print(\" ------------------ \")\n",
    "        \n",
    "#         print(tweet)\n",
    "#         print(status.extended_tweet['full_text'])\n",
    "#         print(tweet.full_text)\n",
    "        #Verify the tweet has place info before writing (It should, if it got past our place filter)\n",
    "        if tweet.place is not None:\n",
    "            \n",
    "            #Write the JSON format to the text file, and add one to the number of tweets we've collected\n",
    "            f.write(\"\")\n",
    "#             print(\"error(test): \" + str(error_test))\n",
    "#             f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "            tweetCount += 1\n",
    "\n",
    "    #Display how many tweets we have collected\n",
    "    print(\"Downloaded {0} tweets\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On Thursdays there are lots of opportunities to take your kids out to an activity and meet other moms. Check them out on the calendar: https://t.co/d4hBa2J62v https://t.co/agCG4KPDLc', 'Facebook refuse to remove post from Tory-supporting page calling for Jeremy Corbyn to be assassinated https://t.co/kJHurNI0HX via @EvolvePolitics', 'I post on Facebook like once every 3 months just to give people a chance to listen to me now and to prepare them for when the book deal comes']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_saved_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "\n",
    "stream_listener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=stream_listner)\n",
    "myStream.filter(track=['facebook', 'cambridge analytics', 'mark zuckerberg', 'zuckerberg'])\n",
    "myStream.filter(track=['facebook'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class listener(tweepy.StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(data)\n",
    "        return(True)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "twitterStream = tweepy.Stream(auth, listener(), tweet_mode='extended')\n",
    "twitterStream.filter(track=[\"#deletefacebook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "searchQuery = '#deletefacebook'  # this is what we're searching for\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'tweets.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            \n",
    "            \n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(new_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_tweet_data = new_tweets.json()\n",
    "new_new_tweets = new_tweets\n",
    "for x in new_new_tweets:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
